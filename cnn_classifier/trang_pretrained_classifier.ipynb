{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import cv2\n",
    "from create_model_trang import create_cnn_model_trang\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor\n",
    "\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import expanduser, exists, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet_class_index.json\n",
      "inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
      "inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Kuszma.JPG\n",
      "resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "xception_weights_tf_dim_ordering_tf_kernels.h5\n",
      "xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    }
   ],
   "source": [
    "# Use keras pretrained models\n",
    "# Source: https://www.kaggle.com/gaborfodor/resnet50-example\n",
    "\n",
    "!ls ../keras-pretrained-models/\n",
    "\n",
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)\n",
    "    \n",
    "!cp ../keras-pretrained-models/*notop* ~/.keras/models/\n",
    "!cp ../keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n",
    "!cp ../keras-pretrained-models/resnet50* ~/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train and test folders\n",
    "train_folder = '../train/'\n",
    "test_folder = '../test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "      <td>../train/000bec180eb18c7604dcecc8fe0dba07.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "      <td>../train/001513dfcb2ffafc82cccf4d8bbaba97.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "      <td>../train/001cdf01b096e06d78e9e5112d419397.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "      <td>../train/00214f311d5d2247d5dfe4fe24b2303d.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "      <td>../train/0021f9ceb3235effd7fcde7f7538ed62.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>002211c81b498ef88e1b40b9abf84e1d</td>\n",
       "      <td>bedlington_terrier</td>\n",
       "      <td>../train/002211c81b498ef88e1b40b9abf84e1d.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00290d3e1fdd27226ba27a8ce248ce85</td>\n",
       "      <td>bedlington_terrier</td>\n",
       "      <td>../train/00290d3e1fdd27226ba27a8ce248ce85.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002a283a315af96eaea0e28e7163b21b</td>\n",
       "      <td>borzoi</td>\n",
       "      <td>../train/002a283a315af96eaea0e28e7163b21b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>003df8b8a8b05244b1d920bb6cf451f9</td>\n",
       "      <td>basenji</td>\n",
       "      <td>../train/003df8b8a8b05244b1d920bb6cf451f9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0042188c895a2f14ef64a918ed9c7b64</td>\n",
       "      <td>scottish_deerhound</td>\n",
       "      <td>../train/0042188c895a2f14ef64a918ed9c7b64.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id               breed  \\\n",
       "0  000bec180eb18c7604dcecc8fe0dba07         boston_bull   \n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97               dingo   \n",
       "2  001cdf01b096e06d78e9e5112d419397            pekinese   \n",
       "3  00214f311d5d2247d5dfe4fe24b2303d            bluetick   \n",
       "4  0021f9ceb3235effd7fcde7f7538ed62    golden_retriever   \n",
       "5  002211c81b498ef88e1b40b9abf84e1d  bedlington_terrier   \n",
       "6  00290d3e1fdd27226ba27a8ce248ce85  bedlington_terrier   \n",
       "7  002a283a315af96eaea0e28e7163b21b              borzoi   \n",
       "8  003df8b8a8b05244b1d920bb6cf451f9             basenji   \n",
       "9  0042188c895a2f14ef64a918ed9c7b64  scottish_deerhound   \n",
       "\n",
       "                                      image_path  \n",
       "0  ../train/000bec180eb18c7604dcecc8fe0dba07.jpg  \n",
       "1  ../train/001513dfcb2ffafc82cccf4d8bbaba97.jpg  \n",
       "2  ../train/001cdf01b096e06d78e9e5112d419397.jpg  \n",
       "3  ../train/00214f311d5d2247d5dfe4fe24b2303d.jpg  \n",
       "4  ../train/0021f9ceb3235effd7fcde7f7538ed62.jpg  \n",
       "5  ../train/002211c81b498ef88e1b40b9abf84e1d.jpg  \n",
       "6  ../train/00290d3e1fdd27226ba27a8ce248ce85.jpg  \n",
       "7  ../train/002a283a315af96eaea0e28e7163b21b.jpg  \n",
       "8  ../train/003df8b8a8b05244b1d920bb6cf451f9.jpg  \n",
       "9  ../train/0042188c895a2f14ef64a918ed9c7b64.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv('../labels.csv')\n",
    "training_data['image_path'] = training_data.apply( lambda x: (train_folder + x[\"id\"] + \".jpg\" ), axis=1)\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group image paths into an array to train on\n",
    "train_data = np.array([img_to_array(load_img(img, target_size=(224, 224))) for img in training_data['image_path'].values.tolist()]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation. Since we only have train and validation folders, need to divide train into training and validation sets. \n",
    "# Save validation folder for later testing\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(train_data, \n",
    "                                                                training_data[\"breed\"], \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=np.array(training_data[\"breed\"]), \n",
    "                                                                random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert the train and validation labels into one hot encoded format\n",
    "y_train = pd.get_dummies(y_train.reset_index(drop=True)).as_matrix()\n",
    "y_validation = pd.get_dummies(y_validation.reset_index(drop=True)).as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, \n",
    "                                   horizontal_flip = True)\n",
    "train_generator = train_datagen.flow(x_train, y_train, shuffle=False, batch_size=20, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation generator\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_generator = train_datagen.flow(x_validation, y_validation, shuffle=False, batch_size=20, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 11:00 - loss: 4.8476 - acc: 0.0000e+ - ETA: 8:09 - loss: 4.7791 - acc: 0.0000e+00 - ETA: 7:03 - loss: 4.8184 - acc: 0.0000e+0 - ETA: 6:26 - loss: 4.8291 - acc: 0.0000e+0 - ETA: 5:59 - loss: 4.8555 - acc: 0.0000e+0 - ETA: 5:43 - loss: 4.8118 - acc: 0.0083    - ETA: 5:28 - loss: 4.8275 - acc: 0.007 - ETA: 5:16 - loss: 4.8918 - acc: 0.006 - ETA: 5:07 - loss: 4.8941 - acc: 0.011 - ETA: 4:59 - loss: 4.8680 - acc: 0.010 - ETA: 4:51 - loss: 4.8776 - acc: 0.009 - ETA: 4:44 - loss: 4.8885 - acc: 0.008 - ETA: 4:37 - loss: 4.8808 - acc: 0.007 - ETA: 4:32 - loss: 4.8673 - acc: 0.007 - ETA: 4:26 - loss: 4.8766 - acc: 0.006 - ETA: 4:21 - loss: 4.8561 - acc: 0.006 - ETA: 4:17 - loss: 4.8807 - acc: 0.005 - ETA: 4:12 - loss: 4.8979 - acc: 0.005 - ETA: 4:08 - loss: 4.8965 - acc: 0.007 - ETA: 4:03 - loss: 4.8892 - acc: 0.010 - ETA: 3:59 - loss: 4.9025 - acc: 0.009 - ETA: 3:56 - loss: 4.9129 - acc: 0.009 - ETA: 3:53 - loss: 4.9297 - acc: 0.008 - ETA: 3:49 - loss: 4.9208 - acc: 0.008 - ETA: 3:45 - loss: 4.9211 - acc: 0.008 - ETA: 3:43 - loss: 4.9270 - acc: 0.011 - ETA: 3:41 - loss: 4.9321 - acc: 0.011 - ETA: 3:39 - loss: 4.9341 - acc: 0.010 - ETA: 3:36 - loss: 4.9323 - acc: 0.012 - ETA: 3:32 - loss: 4.9378 - acc: 0.013 - ETA: 3:31 - loss: 4.9343 - acc: 0.012 - ETA: 3:29 - loss: 4.9410 - acc: 0.012 - ETA: 3:27 - loss: 4.9393 - acc: 0.012 - ETA: 3:24 - loss: 4.9396 - acc: 0.011 - ETA: 3:21 - loss: 4.9461 - acc: 0.012 - ETA: 3:18 - loss: 4.9456 - acc: 0.012 - ETA: 3:14 - loss: 4.9484 - acc: 0.012 - ETA: 3:11 - loss: 4.9591 - acc: 0.011 - ETA: 3:08 - loss: 4.9675 - acc: 0.011 - ETA: 3:04 - loss: 4.9768 - acc: 0.011 - ETA: 3:01 - loss: 4.9721 - acc: 0.011 - ETA: 2:57 - loss: 4.9655 - acc: 0.013 - ETA: 2:54 - loss: 4.9723 - acc: 0.012 - ETA: 2:50 - loss: 4.9664 - acc: 0.014 - ETA: 2:47 - loss: 4.9692 - acc: 0.014 - ETA: 2:44 - loss: 4.9744 - acc: 0.014 - ETA: 2:40 - loss: 4.9734 - acc: 0.013 - ETA: 2:37 - loss: 4.9722 - acc: 0.013 - ETA: 2:34 - loss: 4.9728 - acc: 0.013 - ETA: 2:31 - loss: 4.9641 - acc: 0.014 - ETA: 2:28 - loss: 4.9641 - acc: 0.014 - ETA: 2:24 - loss: 4.9650 - acc: 0.014 - ETA: 2:21 - loss: 4.9665 - acc: 0.014 - ETA: 2:18 - loss: 4.9673 - acc: 0.013 - ETA: 2:15 - loss: 4.9630 - acc: 0.013 - ETA: 2:12 - loss: 4.9616 - acc: 0.013 - ETA: 2:09 - loss: 4.9615 - acc: 0.013 - ETA: 2:05 - loss: 4.9572 - acc: 0.012 - ETA: 2:02 - loss: 4.9579 - acc: 0.012 - ETA: 1:59 - loss: 4.9588 - acc: 0.012 - ETA: 1:56 - loss: 4.9624 - acc: 0.013 - ETA: 1:53 - loss: 4.9606 - acc: 0.013 - ETA: 1:50 - loss: 4.9609 - acc: 0.013 - ETA: 1:47 - loss: 4.9632 - acc: 0.013 - ETA: 1:44 - loss: 4.9604 - acc: 0.013 - ETA: 1:41 - loss: 4.9624 - acc: 0.012 - ETA: 1:38 - loss: 4.9583 - acc: 0.013 - ETA: 1:35 - loss: 4.9594 - acc: 0.013 - ETA: 1:32 - loss: 4.9552 - acc: 0.013 - ETA: 1:29 - loss: 4.9535 - acc: 0.012 - ETA: 1:26 - loss: 4.9526 - acc: 0.012 - ETA: 1:23 - loss: 4.9496 - acc: 0.013 - ETA: 1:20 - loss: 4.9480 - acc: 0.013 - ETA: 1:17 - loss: 4.9477 - acc: 0.012 - ETA: 1:14 - loss: 4.9436 - acc: 0.012 - ETA: 1:11 - loss: 4.9424 - acc: 0.012 - ETA: 1:08 - loss: 4.9404 - acc: 0.012 - ETA: 1:05 - loss: 4.9393 - acc: 0.012 - ETA: 1:02 - loss: 4.9382 - acc: 0.012 - ETA: 59s - loss: 4.9392 - acc: 0.012 - ETA: 56s - loss: 4.9411 - acc: 0.01 - ETA: 53s - loss: 4.9377 - acc: 0.01 - ETA: 50s - loss: 4.9401 - acc: 0.01 - ETA: 47s - loss: 4.9412 - acc: 0.01 - ETA: 44s - loss: 4.9387 - acc: 0.01 - ETA: 41s - loss: 4.9422 - acc: 0.01 - ETA: 38s - loss: 4.9419 - acc: 0.01 - ETA: 35s - loss: 4.9380 - acc: 0.01 - ETA: 32s - loss: 4.9387 - acc: 0.01 - ETA: 29s - loss: 4.9359 - acc: 0.01 - ETA: 26s - loss: 4.9330 - acc: 0.01 - ETA: 23s - loss: 4.9312 - acc: 0.01 - ETA: 20s - loss: 4.9293 - acc: 0.01 - ETA: 17s - loss: 4.9265 - acc: 0.01 - ETA: 14s - loss: 4.9252 - acc: 0.01 - ETA: 11s - loss: 4.9226 - acc: 0.01 - ETA: 8s - loss: 4.9206 - acc: 0.0119 - ETA: 5s - loss: 4.9211 - acc: 0.011 - ETA: 2s - loss: 4.9203 - acc: 0.012 - 328s 3s/step - loss: 4.9185 - acc: 0.0130 - val_loss: 4.9146 - val_acc: 0.0163\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.91465, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 5:00 - loss: 4.8768 - acc: 0.0000e+0 - ETA: 4:46 - loss: 4.9431 - acc: 0.0000e+0 - ETA: 4:40 - loss: 4.8459 - acc: 0.0000e+0 - ETA: 4:36 - loss: 4.8545 - acc: 0.0000e+0 - ETA: 4:32 - loss: 4.8408 - acc: 0.0000e+0 - ETA: 4:28 - loss: 4.8360 - acc: 0.0000e+0 - ETA: 4:25 - loss: 4.8668 - acc: 0.0000e+0 - ETA: 4:22 - loss: 4.8663 - acc: 0.0063    - ETA: 4:18 - loss: 4.8495 - acc: 0.011 - ETA: 4:15 - loss: 4.8368 - acc: 0.010 - ETA: 4:12 - loss: 4.8368 - acc: 0.013 - ETA: 4:09 - loss: 4.8331 - acc: 0.012 - ETA: 4:06 - loss: 4.8118 - acc: 0.015 - ETA: 4:03 - loss: 4.8207 - acc: 0.017 - ETA: 4:00 - loss: 4.8159 - acc: 0.016 - ETA: 3:57 - loss: 4.8400 - acc: 0.015 - ETA: 3:54 - loss: 4.8354 - acc: 0.014 - ETA: 3:51 - loss: 4.8327 - acc: 0.013 - ETA: 3:48 - loss: 4.8423 - acc: 0.013 - ETA: 3:46 - loss: 4.8413 - acc: 0.012 - ETA: 3:43 - loss: 4.8451 - acc: 0.014 - ETA: 3:40 - loss: 4.8377 - acc: 0.015 - ETA: 3:37 - loss: 4.8323 - acc: 0.015 - ETA: 3:34 - loss: 4.8249 - acc: 0.016 - ETA: 3:31 - loss: 4.8227 - acc: 0.018 - ETA: 3:28 - loss: 4.8264 - acc: 0.017 - ETA: 3:26 - loss: 4.8221 - acc: 0.018 - ETA: 3:23 - loss: 4.8309 - acc: 0.019 - ETA: 3:21 - loss: 4.8288 - acc: 0.020 - ETA: 3:18 - loss: 4.8311 - acc: 0.020 - ETA: 3:15 - loss: 4.8179 - acc: 0.022 - ETA: 3:12 - loss: 4.8189 - acc: 0.021 - ETA: 3:09 - loss: 4.8275 - acc: 0.021 - ETA: 3:06 - loss: 4.8247 - acc: 0.022 - ETA: 3:03 - loss: 4.8255 - acc: 0.024 - ETA: 3:00 - loss: 4.8280 - acc: 0.025 - ETA: 2:58 - loss: 4.8284 - acc: 0.024 - ETA: 2:55 - loss: 4.8310 - acc: 0.023 - ETA: 2:52 - loss: 4.8336 - acc: 0.023 - ETA: 2:49 - loss: 4.8334 - acc: 0.023 - ETA: 2:46 - loss: 4.8294 - acc: 0.023 - ETA: 2:44 - loss: 4.8255 - acc: 0.023 - ETA: 2:41 - loss: 4.8314 - acc: 0.023 - ETA: 2:38 - loss: 4.8321 - acc: 0.022 - ETA: 2:35 - loss: 4.8339 - acc: 0.022 - ETA: 2:32 - loss: 4.8308 - acc: 0.021 - ETA: 2:30 - loss: 4.8304 - acc: 0.021 - ETA: 2:27 - loss: 4.8277 - acc: 0.021 - ETA: 2:24 - loss: 4.8240 - acc: 0.024 - ETA: 2:21 - loss: 4.8193 - acc: 0.024 - ETA: 2:18 - loss: 4.8214 - acc: 0.023 - ETA: 2:15 - loss: 4.8160 - acc: 0.023 - ETA: 2:12 - loss: 4.8123 - acc: 0.025 - ETA: 2:10 - loss: 4.8152 - acc: 0.025 - ETA: 2:07 - loss: 4.8167 - acc: 0.024 - ETA: 2:04 - loss: 4.8149 - acc: 0.024 - ETA: 2:01 - loss: 4.8132 - acc: 0.023 - ETA: 1:58 - loss: 4.8122 - acc: 0.023 - ETA: 1:56 - loss: 4.8164 - acc: 0.022 - ETA: 1:53 - loss: 4.8201 - acc: 0.022 - ETA: 1:50 - loss: 4.8218 - acc: 0.022 - ETA: 1:47 - loss: 4.8229 - acc: 0.021 - ETA: 1:45 - loss: 4.8198 - acc: 0.022 - ETA: 1:42 - loss: 4.8133 - acc: 0.025 - ETA: 1:39 - loss: 4.8100 - acc: 0.024 - ETA: 1:36 - loss: 4.8073 - acc: 0.024 - ETA: 1:33 - loss: 4.8045 - acc: 0.023 - ETA: 1:30 - loss: 4.8041 - acc: 0.023 - ETA: 1:27 - loss: 4.8024 - acc: 0.023 - ETA: 1:25 - loss: 4.8065 - acc: 0.023 - ETA: 1:22 - loss: 4.8056 - acc: 0.023 - ETA: 1:19 - loss: 4.8018 - acc: 0.024 - ETA: 1:16 - loss: 4.8066 - acc: 0.024 - ETA: 1:13 - loss: 4.8110 - acc: 0.023 - ETA: 1:10 - loss: 4.8146 - acc: 0.023 - ETA: 1:08 - loss: 4.8161 - acc: 0.023 - ETA: 1:05 - loss: 4.8155 - acc: 0.024 - ETA: 1:02 - loss: 4.8194 - acc: 0.023 - ETA: 59s - loss: 4.8140 - acc: 0.024 - ETA: 56s - loss: 4.8098 - acc: 0.02 - ETA: 53s - loss: 4.8060 - acc: 0.02 - ETA: 50s - loss: 4.8058 - acc: 0.02 - ETA: 48s - loss: 4.8058 - acc: 0.02 - ETA: 45s - loss: 4.8045 - acc: 0.02 - ETA: 42s - loss: 4.8033 - acc: 0.02 - ETA: 39s - loss: 4.8009 - acc: 0.02 - ETA: 36s - loss: 4.8006 - acc: 0.02 - ETA: 33s - loss: 4.8001 - acc: 0.02 - ETA: 31s - loss: 4.7977 - acc: 0.02 - ETA: 28s - loss: 4.7984 - acc: 0.02 - ETA: 25s - loss: 4.7971 - acc: 0.02 - ETA: 22s - loss: 4.7956 - acc: 0.02 - ETA: 19s - loss: 4.7948 - acc: 0.02 - ETA: 16s - loss: 4.7963 - acc: 0.02 - ETA: 14s - loss: 4.7988 - acc: 0.02 - ETA: 11s - loss: 4.8002 - acc: 0.02 - ETA: 8s - loss: 4.8029 - acc: 0.0222 - ETA: 5s - loss: 4.8016 - acc: 0.021 - ETA: 2s - loss: 4.8028 - acc: 0.022 - 318s 3s/step - loss: 4.8025 - acc: 0.0220 - val_loss: 4.8293 - val_acc: 0.0238\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.91465 to 4.82928, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - ETA: 5:28 - loss: 4.6724 - acc: 0.0000e+0 - ETA: 5:09 - loss: 4.6649 - acc: 0.0250    - ETA: 4:59 - loss: 4.7308 - acc: 0.016 - ETA: 4:53 - loss: 4.6992 - acc: 0.025 - ETA: 4:47 - loss: 4.6663 - acc: 0.020 - ETA: 4:42 - loss: 4.6552 - acc: 0.033 - ETA: 4:37 - loss: 4.6564 - acc: 0.028 - ETA: 4:33 - loss: 4.6661 - acc: 0.031 - ETA: 4:30 - loss: 4.6539 - acc: 0.027 - ETA: 4:26 - loss: 4.6624 - acc: 0.025 - ETA: 4:22 - loss: 4.6760 - acc: 0.022 - ETA: 4:18 - loss: 4.6708 - acc: 0.020 - ETA: 4:15 - loss: 4.6862 - acc: 0.019 - ETA: 4:11 - loss: 4.7141 - acc: 0.017 - ETA: 4:07 - loss: 4.6883 - acc: 0.023 - ETA: 4:04 - loss: 4.7061 - acc: 0.021 - ETA: 4:00 - loss: 4.7079 - acc: 0.020 - ETA: 3:57 - loss: 4.7249 - acc: 0.019 - ETA: 3:54 - loss: 4.7332 - acc: 0.018 - ETA: 3:51 - loss: 4.7405 - acc: 0.017 - ETA: 3:48 - loss: 4.7259 - acc: 0.016 - ETA: 3:45 - loss: 4.7183 - acc: 0.015 - ETA: 3:42 - loss: 4.7221 - acc: 0.015 - ETA: 3:39 - loss: 4.7158 - acc: 0.016 - ETA: 3:36 - loss: 4.7073 - acc: 0.016 - ETA: 3:33 - loss: 4.7109 - acc: 0.015 - ETA: 3:30 - loss: 4.7122 - acc: 0.016 - ETA: 3:26 - loss: 4.7123 - acc: 0.016 - ETA: 3:23 - loss: 4.7205 - acc: 0.015 - ETA: 3:20 - loss: 4.7265 - acc: 0.015 - ETA: 3:17 - loss: 4.7226 - acc: 0.016 - ETA: 3:14 - loss: 4.7254 - acc: 0.015 - ETA: 3:11 - loss: 4.7204 - acc: 0.016 - ETA: 3:08 - loss: 4.7211 - acc: 0.016 - ETA: 3:05 - loss: 4.7208 - acc: 0.015 - ETA: 3:02 - loss: 4.7263 - acc: 0.016 - ETA: 2:59 - loss: 4.7280 - acc: 0.016 - ETA: 2:56 - loss: 4.7330 - acc: 0.015 - ETA: 2:54 - loss: 4.7310 - acc: 0.015 - ETA: 2:51 - loss: 4.7248 - acc: 0.016 - ETA: 2:48 - loss: 4.7204 - acc: 0.015 - ETA: 2:45 - loss: 4.7221 - acc: 0.016 - ETA: 2:42 - loss: 4.7279 - acc: 0.016 - ETA: 2:39 - loss: 4.7332 - acc: 0.015 - ETA: 2:36 - loss: 4.7294 - acc: 0.015 - ETA: 2:33 - loss: 4.7277 - acc: 0.016 - ETA: 2:31 - loss: 4.7267 - acc: 0.016 - ETA: 2:28 - loss: 4.7274 - acc: 0.015 - ETA: 2:25 - loss: 4.7296 - acc: 0.016 - ETA: 2:22 - loss: 4.7328 - acc: 0.016 - ETA: 2:20 - loss: 4.7343 - acc: 0.015 - ETA: 2:17 - loss: 4.7343 - acc: 0.016 - ETA: 2:14 - loss: 4.7354 - acc: 0.017 - ETA: 2:11 - loss: 4.7392 - acc: 0.016 - ETA: 2:08 - loss: 4.7374 - acc: 0.017 - ETA: 2:05 - loss: 4.7352 - acc: 0.017 - ETA: 2:03 - loss: 4.7359 - acc: 0.017 - ETA: 2:00 - loss: 4.7350 - acc: 0.017 - ETA: 1:57 - loss: 4.7362 - acc: 0.016 - ETA: 1:54 - loss: 4.7335 - acc: 0.018 - ETA: 1:51 - loss: 4.7309 - acc: 0.018 - ETA: 1:48 - loss: 4.7277 - acc: 0.017 - ETA: 1:45 - loss: 4.7278 - acc: 0.018 - ETA: 1:42 - loss: 4.7308 - acc: 0.018 - ETA: 1:40 - loss: 4.7326 - acc: 0.018 - ETA: 1:37 - loss: 4.7280 - acc: 0.019 - ETA: 1:34 - loss: 4.7275 - acc: 0.020 - ETA: 1:31 - loss: 4.7271 - acc: 0.019 - ETA: 1:28 - loss: 4.7250 - acc: 0.020 - ETA: 1:25 - loss: 4.7240 - acc: 0.020 - ETA: 1:22 - loss: 4.7266 - acc: 0.020 - ETA: 1:19 - loss: 4.7244 - acc: 0.020 - ETA: 1:17 - loss: 4.7260 - acc: 0.020 - ETA: 1:14 - loss: 4.7204 - acc: 0.020 - ETA: 1:11 - loss: 4.7172 - acc: 0.020 - ETA: 1:08 - loss: 4.7187 - acc: 0.021 - ETA: 1:05 - loss: 4.7212 - acc: 0.021 - ETA: 1:02 - loss: 4.7193 - acc: 0.021 - ETA: 59s - loss: 4.7196 - acc: 0.021 - ETA: 57s - loss: 4.7168 - acc: 0.02 - ETA: 54s - loss: 4.7154 - acc: 0.02 - ETA: 51s - loss: 4.7147 - acc: 0.02 - ETA: 48s - loss: 4.7121 - acc: 0.02 - ETA: 45s - loss: 4.7133 - acc: 0.02 - ETA: 42s - loss: 4.7118 - acc: 0.02 - ETA: 39s - loss: 4.7088 - acc: 0.02 - ETA: 37s - loss: 4.7069 - acc: 0.02 - ETA: 34s - loss: 4.7063 - acc: 0.02 - ETA: 31s - loss: 4.7039 - acc: 0.02 - ETA: 28s - loss: 4.7039 - acc: 0.02 - ETA: 25s - loss: 4.7044 - acc: 0.02 - ETA: 22s - loss: 4.7052 - acc: 0.02 - ETA: 19s - loss: 4.7048 - acc: 0.02 - ETA: 17s - loss: 4.7057 - acc: 0.02 - ETA: 14s - loss: 4.7074 - acc: 0.02 - ETA: 11s - loss: 4.7069 - acc: 0.02 - ETA: 8s - loss: 4.7037 - acc: 0.0253 - ETA: 5s - loss: 4.7062 - acc: 0.025 - ETA: 2s - loss: 4.7059 - acc: 0.024 - 320s 3s/step - loss: 4.7022 - acc: 0.0245 - val_loss: 4.7659 - val_acc: 0.0306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 4.82928 to 4.76593, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - ETA: 5:16 - loss: 4.6303 - acc: 0.0000e+0 - ETA: 5:13 - loss: 4.6580 - acc: 0.0000e+0 - ETA: 5:04 - loss: 4.6795 - acc: 0.0000e+0 - ETA: 4:55 - loss: 4.6999 - acc: 0.0125    - ETA: 4:48 - loss: 4.7044 - acc: 0.010 - ETA: 4:44 - loss: 4.7061 - acc: 0.016 - ETA: 4:39 - loss: 4.7091 - acc: 0.014 - ETA: 4:34 - loss: 4.7144 - acc: 0.012 - ETA: 4:29 - loss: 4.7044 - acc: 0.016 - ETA: 4:26 - loss: 4.7162 - acc: 0.015 - ETA: 4:21 - loss: 4.7202 - acc: 0.013 - ETA: 4:18 - loss: 4.7117 - acc: 0.016 - ETA: 4:16 - loss: 4.7104 - acc: 0.015 - ETA: 4:12 - loss: 4.7185 - acc: 0.014 - ETA: 4:08 - loss: 4.6989 - acc: 0.013 - ETA: 4:05 - loss: 4.6803 - acc: 0.012 - ETA: 4:02 - loss: 4.6725 - acc: 0.011 - ETA: 3:58 - loss: 4.6704 - acc: 0.011 - ETA: 3:55 - loss: 4.6795 - acc: 0.010 - ETA: 3:52 - loss: 4.6783 - acc: 0.010 - ETA: 3:48 - loss: 4.6777 - acc: 0.009 - ETA: 3:45 - loss: 4.6680 - acc: 0.011 - ETA: 3:42 - loss: 4.6544 - acc: 0.015 - ETA: 3:39 - loss: 4.6602 - acc: 0.014 - ETA: 3:36 - loss: 4.6532 - acc: 0.014 - ETA: 3:33 - loss: 4.6570 - acc: 0.013 - ETA: 3:30 - loss: 4.6643 - acc: 0.013 - ETA: 3:27 - loss: 4.6621 - acc: 0.014 - ETA: 3:24 - loss: 4.6589 - acc: 0.017 - ETA: 3:21 - loss: 4.6563 - acc: 0.016 - ETA: 3:17 - loss: 4.6570 - acc: 0.016 - ETA: 3:14 - loss: 4.6519 - acc: 0.017 - ETA: 3:11 - loss: 4.6534 - acc: 0.016 - ETA: 3:08 - loss: 4.6553 - acc: 0.016 - ETA: 3:06 - loss: 4.6549 - acc: 0.015 - ETA: 3:03 - loss: 4.6469 - acc: 0.018 - ETA: 3:00 - loss: 4.6436 - acc: 0.018 - ETA: 2:57 - loss: 4.6431 - acc: 0.021 - ETA: 2:54 - loss: 4.6406 - acc: 0.020 - ETA: 2:51 - loss: 4.6436 - acc: 0.020 - ETA: 2:48 - loss: 4.6474 - acc: 0.020 - ETA: 2:45 - loss: 4.6328 - acc: 0.022 - ETA: 2:42 - loss: 4.6377 - acc: 0.022 - ETA: 2:39 - loss: 4.6411 - acc: 0.021 - ETA: 2:36 - loss: 4.6452 - acc: 0.021 - ETA: 2:33 - loss: 4.6376 - acc: 0.022 - ETA: 2:30 - loss: 4.6366 - acc: 0.024 - ETA: 2:28 - loss: 4.6402 - acc: 0.024 - ETA: 2:25 - loss: 4.6333 - acc: 0.024 - ETA: 2:22 - loss: 4.6282 - acc: 0.025 - ETA: 2:19 - loss: 4.6255 - acc: 0.026 - ETA: 2:16 - loss: 4.6267 - acc: 0.026 - ETA: 2:14 - loss: 4.6243 - acc: 0.026 - ETA: 2:11 - loss: 4.6274 - acc: 0.026 - ETA: 2:08 - loss: 4.6290 - acc: 0.026 - ETA: 2:05 - loss: 4.6308 - acc: 0.025 - ETA: 2:02 - loss: 4.6324 - acc: 0.025 - ETA: 1:59 - loss: 4.6233 - acc: 0.026 - ETA: 1:56 - loss: 4.6244 - acc: 0.027 - ETA: 1:54 - loss: 4.6272 - acc: 0.026 - ETA: 1:51 - loss: 4.6235 - acc: 0.027 - ETA: 1:48 - loss: 4.6284 - acc: 0.027 - ETA: 1:45 - loss: 4.6307 - acc: 0.027 - ETA: 1:42 - loss: 4.6258 - acc: 0.026 - ETA: 1:40 - loss: 4.6254 - acc: 0.026 - ETA: 1:37 - loss: 4.6279 - acc: 0.027 - ETA: 1:34 - loss: 4.6279 - acc: 0.026 - ETA: 1:32 - loss: 4.6270 - acc: 0.027 - ETA: 1:29 - loss: 4.6284 - acc: 0.028 - ETA: 1:26 - loss: 4.6293 - acc: 0.027 - ETA: 1:23 - loss: 4.6281 - acc: 0.027 - ETA: 1:20 - loss: 4.6236 - acc: 0.027 - ETA: 1:18 - loss: 4.6271 - acc: 0.026 - ETA: 1:15 - loss: 4.6257 - acc: 0.026 - ETA: 1:12 - loss: 4.6230 - acc: 0.026 - ETA: 1:09 - loss: 4.6226 - acc: 0.026 - ETA: 1:06 - loss: 4.6149 - acc: 0.026 - ETA: 1:03 - loss: 4.6130 - acc: 0.026 - ETA: 1:01 - loss: 4.6115 - acc: 0.025 - ETA: 58s - loss: 4.6105 - acc: 0.026 - ETA: 55s - loss: 4.6101 - acc: 0.02 - ETA: 52s - loss: 4.6071 - acc: 0.02 - ETA: 49s - loss: 4.6038 - acc: 0.02 - ETA: 46s - loss: 4.6042 - acc: 0.02 - ETA: 43s - loss: 4.6020 - acc: 0.02 - ETA: 40s - loss: 4.6031 - acc: 0.02 - ETA: 37s - loss: 4.6013 - acc: 0.02 - ETA: 34s - loss: 4.6020 - acc: 0.02 - ETA: 31s - loss: 4.6048 - acc: 0.02 - ETA: 29s - loss: 4.6040 - acc: 0.02 - ETA: 26s - loss: 4.6083 - acc: 0.02 - ETA: 23s - loss: 4.6093 - acc: 0.02 - ETA: 20s - loss: 4.6122 - acc: 0.02 - ETA: 17s - loss: 4.6107 - acc: 0.02 - ETA: 14s - loss: 4.6103 - acc: 0.02 - ETA: 11s - loss: 4.6080 - acc: 0.02 - ETA: 8s - loss: 4.6048 - acc: 0.0268 - ETA: 5s - loss: 4.6020 - acc: 0.027 - ETA: 2s - loss: 4.5987 - acc: 0.028 - 326s 3s/step - loss: 4.5988 - acc: 0.0290 - val_loss: 4.7700 - val_acc: 0.0363\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.76593\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - ETA: 5:21 - loss: 4.7482 - acc: 0.0000e+0 - ETA: 5:11 - loss: 4.7399 - acc: 0.0000e+0 - ETA: 4:57 - loss: 4.7549 - acc: 0.0167    - ETA: 4:50 - loss: 4.6728 - acc: 0.037 - ETA: 4:44 - loss: 4.6349 - acc: 0.050 - ETA: 4:38 - loss: 4.6751 - acc: 0.041 - ETA: 4:33 - loss: 4.6272 - acc: 0.050 - ETA: 4:29 - loss: 4.5892 - acc: 0.050 - ETA: 4:21 - loss: 4.5675 - acc: 0.057 - ETA: 4:17 - loss: 4.5358 - acc: 0.066 - ETA: 4:14 - loss: 4.5190 - acc: 0.060 - ETA: 4:11 - loss: 4.5090 - acc: 0.064 - ETA: 4:07 - loss: 4.5000 - acc: 0.066 - ETA: 4:04 - loss: 4.5360 - acc: 0.062 - ETA: 4:01 - loss: 4.5608 - acc: 0.057 - ETA: 3:58 - loss: 4.5593 - acc: 0.057 - ETA: 3:55 - loss: 4.5618 - acc: 0.054 - ETA: 3:52 - loss: 4.5678 - acc: 0.051 - ETA: 3:49 - loss: 4.5561 - acc: 0.050 - ETA: 3:46 - loss: 4.5553 - acc: 0.048 - ETA: 3:43 - loss: 4.5471 - acc: 0.046 - ETA: 3:40 - loss: 4.5520 - acc: 0.044 - ETA: 3:37 - loss: 4.5421 - acc: 0.044 - ETA: 3:34 - loss: 4.5367 - acc: 0.044 - ETA: 3:31 - loss: 4.5274 - acc: 0.044 - ETA: 3:29 - loss: 4.5358 - acc: 0.043 - ETA: 3:26 - loss: 4.5348 - acc: 0.045 - ETA: 3:23 - loss: 4.5365 - acc: 0.047 - ETA: 3:20 - loss: 4.5387 - acc: 0.045 - ETA: 3:17 - loss: 4.5361 - acc: 0.045 - ETA: 3:15 - loss: 4.5486 - acc: 0.044 - ETA: 3:12 - loss: 4.5496 - acc: 0.042 - ETA: 3:09 - loss: 4.5397 - acc: 0.043 - ETA: 3:06 - loss: 4.5395 - acc: 0.043 - ETA: 3:03 - loss: 4.5335 - acc: 0.043 - ETA: 3:00 - loss: 4.5350 - acc: 0.043 - ETA: 2:58 - loss: 4.5312 - acc: 0.043 - ETA: 2:55 - loss: 4.5354 - acc: 0.042 - ETA: 2:53 - loss: 4.5354 - acc: 0.042 - ETA: 2:50 - loss: 4.5348 - acc: 0.041 - ETA: 2:47 - loss: 4.5392 - acc: 0.041 - ETA: 2:45 - loss: 4.5423 - acc: 0.042 - ETA: 2:42 - loss: 4.5441 - acc: 0.042 - ETA: 2:39 - loss: 4.5417 - acc: 0.042 - ETA: 2:36 - loss: 4.5386 - acc: 0.042 - ETA: 2:33 - loss: 4.5440 - acc: 0.041 - ETA: 2:30 - loss: 4.5561 - acc: 0.040 - ETA: 2:27 - loss: 4.5648 - acc: 0.040 - ETA: 2:25 - loss: 4.5632 - acc: 0.039 - ETA: 2:22 - loss: 4.5677 - acc: 0.038 - ETA: 2:19 - loss: 4.5701 - acc: 0.037 - ETA: 2:16 - loss: 4.5693 - acc: 0.037 - ETA: 2:13 - loss: 4.5660 - acc: 0.037 - ETA: 2:10 - loss: 4.5660 - acc: 0.039 - ETA: 2:07 - loss: 4.5672 - acc: 0.039 - ETA: 2:05 - loss: 4.5656 - acc: 0.038 - ETA: 2:02 - loss: 4.5680 - acc: 0.038 - ETA: 1:59 - loss: 4.5684 - acc: 0.038 - ETA: 1:56 - loss: 4.5609 - acc: 0.039 - ETA: 1:53 - loss: 4.5608 - acc: 0.039 - ETA: 1:50 - loss: 4.5639 - acc: 0.039 - ETA: 1:47 - loss: 4.5620 - acc: 0.040 - ETA: 1:45 - loss: 4.5642 - acc: 0.040 - ETA: 1:42 - loss: 4.5591 - acc: 0.040 - ETA: 1:39 - loss: 4.5577 - acc: 0.039 - ETA: 1:36 - loss: 4.5557 - acc: 0.038 - ETA: 1:33 - loss: 4.5543 - acc: 0.039 - ETA: 1:30 - loss: 4.5567 - acc: 0.038 - ETA: 1:27 - loss: 4.5569 - acc: 0.038 - ETA: 1:25 - loss: 4.5594 - acc: 0.038 - ETA: 1:22 - loss: 4.5613 - acc: 0.038 - ETA: 1:19 - loss: 4.5608 - acc: 0.037 - ETA: 1:16 - loss: 4.5616 - acc: 0.037 - ETA: 1:13 - loss: 4.5596 - acc: 0.037 - ETA: 1:10 - loss: 4.5579 - acc: 0.036 - ETA: 1:08 - loss: 4.5564 - acc: 0.037 - ETA: 1:05 - loss: 4.5558 - acc: 0.036 - ETA: 1:02 - loss: 4.5526 - acc: 0.036 - ETA: 59s - loss: 4.5496 - acc: 0.036 - ETA: 56s - loss: 4.5500 - acc: 0.03 - ETA: 53s - loss: 4.5505 - acc: 0.03 - ETA: 51s - loss: 4.5479 - acc: 0.03 - ETA: 48s - loss: 4.5507 - acc: 0.03 - ETA: 45s - loss: 4.5493 - acc: 0.03 - ETA: 42s - loss: 4.5522 - acc: 0.03 - ETA: 39s - loss: 4.5528 - acc: 0.03 - ETA: 36s - loss: 4.5525 - acc: 0.03 - ETA: 34s - loss: 4.5492 - acc: 0.03 - ETA: 31s - loss: 4.5516 - acc: 0.03 - ETA: 28s - loss: 4.5498 - acc: 0.03 - ETA: 25s - loss: 4.5478 - acc: 0.03 - ETA: 22s - loss: 4.5512 - acc: 0.03 - ETA: 19s - loss: 4.5503 - acc: 0.03 - ETA: 17s - loss: 4.5480 - acc: 0.03 - ETA: 14s - loss: 4.5479 - acc: 0.03 - ETA: 11s - loss: 4.5451 - acc: 0.03 - ETA: 8s - loss: 4.5462 - acc: 0.0357 - ETA: 5s - loss: 4.5479 - acc: 0.035 - ETA: 2s - loss: 4.5469 - acc: 0.035 - 318s 3s/step - loss: 4.5464 - acc: 0.0362 - val_loss: 4.7875 - val_acc: 0.0275\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.76593\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - ETA: 5:24 - loss: 4.3295 - acc: 0.050 - ETA: 5:12 - loss: 4.5526 - acc: 0.025 - ETA: 5:03 - loss: 4.5537 - acc: 0.033 - ETA: 4:55 - loss: 4.4707 - acc: 0.037 - ETA: 4:50 - loss: 4.5163 - acc: 0.040 - ETA: 4:45 - loss: 4.5393 - acc: 0.033 - ETA: 4:39 - loss: 4.5645 - acc: 0.035 - ETA: 4:36 - loss: 4.5685 - acc: 0.031 - ETA: 4:32 - loss: 4.6082 - acc: 0.033 - ETA: 4:29 - loss: 4.6292 - acc: 0.030 - ETA: 4:25 - loss: 4.6664 - acc: 0.027 - ETA: 4:21 - loss: 4.6474 - acc: 0.025 - ETA: 4:17 - loss: 4.6371 - acc: 0.026 - ETA: 4:13 - loss: 4.6292 - acc: 0.025 - ETA: 4:10 - loss: 4.6253 - acc: 0.023 - ETA: 4:06 - loss: 4.6272 - acc: 0.028 - ETA: 4:03 - loss: 4.6165 - acc: 0.026 - ETA: 4:00 - loss: 4.6161 - acc: 0.025 - ETA: 3:56 - loss: 4.6101 - acc: 0.026 - ETA: 3:53 - loss: 4.6180 - acc: 0.027 - ETA: 3:50 - loss: 4.6136 - acc: 0.026 - ETA: 3:47 - loss: 4.6097 - acc: 0.025 - ETA: 3:43 - loss: 4.6083 - acc: 0.023 - ETA: 3:40 - loss: 4.6155 - acc: 0.022 - ETA: 3:37 - loss: 4.6206 - acc: 0.022 - ETA: 3:34 - loss: 4.6155 - acc: 0.025 - ETA: 3:31 - loss: 4.6086 - acc: 0.025 - ETA: 3:28 - loss: 4.6134 - acc: 0.025 - ETA: 3:25 - loss: 4.6132 - acc: 0.024 - ETA: 3:22 - loss: 4.6061 - acc: 0.025 - ETA: 3:19 - loss: 4.5960 - acc: 0.025 - ETA: 3:17 - loss: 4.5887 - acc: 0.026 - ETA: 3:14 - loss: 4.5857 - acc: 0.027 - ETA: 3:11 - loss: 4.5839 - acc: 0.026 - ETA: 3:08 - loss: 4.5877 - acc: 0.027 - ETA: 3:04 - loss: 4.5904 - acc: 0.027 - ETA: 3:01 - loss: 4.5832 - acc: 0.027 - ETA: 2:58 - loss: 4.5789 - acc: 0.027 - ETA: 2:55 - loss: 4.5819 - acc: 0.028 - ETA: 2:52 - loss: 4.5708 - acc: 0.028 - ETA: 2:49 - loss: 4.5668 - acc: 0.029 - ETA: 2:47 - loss: 4.5766 - acc: 0.028 - ETA: 2:44 - loss: 4.5664 - acc: 0.031 - ETA: 2:41 - loss: 4.5651 - acc: 0.030 - ETA: 2:38 - loss: 4.5631 - acc: 0.033 - ETA: 2:35 - loss: 4.5636 - acc: 0.032 - ETA: 2:32 - loss: 4.5600 - acc: 0.033 - ETA: 2:29 - loss: 4.5566 - acc: 0.034 - ETA: 2:26 - loss: 4.5510 - acc: 0.033 - ETA: 2:23 - loss: 4.5436 - acc: 0.035 - ETA: 2:20 - loss: 4.5372 - acc: 0.034 - ETA: 2:17 - loss: 4.5371 - acc: 0.033 - ETA: 2:14 - loss: 4.5370 - acc: 0.033 - ETA: 2:12 - loss: 4.5348 - acc: 0.032 - ETA: 2:09 - loss: 4.5351 - acc: 0.032 - ETA: 2:06 - loss: 4.5334 - acc: 0.032 - ETA: 2:03 - loss: 4.5326 - acc: 0.032 - ETA: 2:00 - loss: 4.5372 - acc: 0.032 - ETA: 1:57 - loss: 4.5310 - acc: 0.032 - ETA: 1:54 - loss: 4.5361 - acc: 0.031 - ETA: 1:51 - loss: 4.5333 - acc: 0.032 - ETA: 1:48 - loss: 4.5266 - acc: 0.033 - ETA: 1:45 - loss: 4.5234 - acc: 0.034 - ETA: 1:43 - loss: 4.5249 - acc: 0.034 - ETA: 1:40 - loss: 4.5267 - acc: 0.034 - ETA: 1:37 - loss: 4.5234 - acc: 0.035 - ETA: 1:34 - loss: 4.5183 - acc: 0.035 - ETA: 1:31 - loss: 4.5181 - acc: 0.035 - ETA: 1:28 - loss: 4.5204 - acc: 0.035 - ETA: 1:25 - loss: 4.5214 - acc: 0.035 - ETA: 1:23 - loss: 4.5181 - acc: 0.036 - ETA: 1:20 - loss: 4.5167 - acc: 0.036 - ETA: 1:17 - loss: 4.5128 - acc: 0.036 - ETA: 1:14 - loss: 4.5044 - acc: 0.036 - ETA: 1:11 - loss: 4.5023 - acc: 0.036 - ETA: 1:08 - loss: 4.5004 - acc: 0.036 - ETA: 1:05 - loss: 4.4972 - acc: 0.037 - ETA: 1:03 - loss: 4.4968 - acc: 0.037 - ETA: 1:00 - loss: 4.5035 - acc: 0.037 - ETA: 57s - loss: 4.5042 - acc: 0.037 - ETA: 54s - loss: 4.5043 - acc: 0.03 - ETA: 51s - loss: 4.5039 - acc: 0.03 - ETA: 48s - loss: 4.5065 - acc: 0.03 - ETA: 45s - loss: 4.5132 - acc: 0.03 - ETA: 43s - loss: 4.5114 - acc: 0.03 - ETA: 40s - loss: 4.5092 - acc: 0.03 - ETA: 37s - loss: 4.5128 - acc: 0.03 - ETA: 34s - loss: 4.5077 - acc: 0.03 - ETA: 31s - loss: 4.5084 - acc: 0.03 - ETA: 28s - loss: 4.5041 - acc: 0.03 - ETA: 25s - loss: 4.5047 - acc: 0.03 - ETA: 22s - loss: 4.5055 - acc: 0.03 - ETA: 20s - loss: 4.5060 - acc: 0.03 - ETA: 17s - loss: 4.5060 - acc: 0.03 - ETA: 14s - loss: 4.5026 - acc: 0.03 - ETA: 11s - loss: 4.5008 - acc: 0.03 - ETA: 8s - loss: 4.5005 - acc: 0.0376 - ETA: 5s - loss: 4.4999 - acc: 0.038 - ETA: 2s - loss: 4.4997 - acc: 0.037 - 322s 3s/step - loss: 4.4962 - acc: 0.0375 - val_loss: 4.5164 - val_acc: 0.0420\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: val_loss improved from 4.76593 to 4.51640, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - ETA: 5:18 - loss: 4.8169 - acc: 0.0000e+0 - ETA: 4:58 - loss: 4.5896 - acc: 0.0000e+0 - ETA: 4:47 - loss: 4.6226 - acc: 0.0000e+0 - ETA: 4:41 - loss: 4.5976 - acc: 0.0000e+0 - ETA: 4:35 - loss: 4.5663 - acc: 0.0000e+0 - ETA: 4:31 - loss: 4.5453 - acc: 0.0083    - ETA: 4:27 - loss: 4.5289 - acc: 0.007 - ETA: 4:23 - loss: 4.5460 - acc: 0.006 - ETA: 4:20 - loss: 4.5491 - acc: 0.011 - ETA: 4:16 - loss: 4.5295 - acc: 0.010 - ETA: 4:13 - loss: 4.4968 - acc: 0.027 - ETA: 4:10 - loss: 4.4894 - acc: 0.025 - ETA: 4:07 - loss: 4.4696 - acc: 0.026 - ETA: 4:04 - loss: 4.4579 - acc: 0.028 - ETA: 4:01 - loss: 4.4358 - acc: 0.030 - ETA: 3:58 - loss: 4.4248 - acc: 0.031 - ETA: 3:55 - loss: 4.4172 - acc: 0.032 - ETA: 3:52 - loss: 4.4235 - acc: 0.030 - ETA: 3:49 - loss: 4.4301 - acc: 0.031 - ETA: 3:46 - loss: 4.4315 - acc: 0.030 - ETA: 3:43 - loss: 4.4394 - acc: 0.028 - ETA: 3:40 - loss: 4.4302 - acc: 0.027 - ETA: 3:38 - loss: 4.4532 - acc: 0.028 - ETA: 3:35 - loss: 4.4376 - acc: 0.029 - ETA: 3:33 - loss: 4.4632 - acc: 0.028 - ETA: 3:30 - loss: 4.4589 - acc: 0.028 - ETA: 3:27 - loss: 4.4622 - acc: 0.029 - ETA: 3:25 - loss: 4.4649 - acc: 0.030 - ETA: 3:22 - loss: 4.4723 - acc: 0.029 - ETA: 3:19 - loss: 4.4705 - acc: 0.030 - ETA: 3:16 - loss: 4.4581 - acc: 0.032 - ETA: 3:13 - loss: 4.4553 - acc: 0.032 - ETA: 3:10 - loss: 4.4518 - acc: 0.033 - ETA: 3:07 - loss: 4.4500 - acc: 0.035 - ETA: 3:05 - loss: 4.4531 - acc: 0.035 - ETA: 3:02 - loss: 4.4629 - acc: 0.036 - ETA: 2:59 - loss: 4.4517 - acc: 0.036 - ETA: 2:56 - loss: 4.4608 - acc: 0.036 - ETA: 2:53 - loss: 4.4616 - acc: 0.035 - ETA: 2:50 - loss: 4.4687 - acc: 0.035 - ETA: 2:47 - loss: 4.4737 - acc: 0.034 - ETA: 2:44 - loss: 4.4720 - acc: 0.033 - ETA: 2:41 - loss: 4.4666 - acc: 0.032 - ETA: 2:39 - loss: 4.4658 - acc: 0.033 - ETA: 2:36 - loss: 4.4717 - acc: 0.032 - ETA: 2:33 - loss: 4.4736 - acc: 0.031 - ETA: 2:30 - loss: 4.4702 - acc: 0.031 - ETA: 2:27 - loss: 4.4625 - acc: 0.033 - ETA: 2:24 - loss: 4.4653 - acc: 0.034 - ETA: 2:22 - loss: 4.4592 - acc: 0.036 - ETA: 2:19 - loss: 4.4613 - acc: 0.036 - ETA: 2:16 - loss: 4.4697 - acc: 0.035 - ETA: 2:13 - loss: 4.4741 - acc: 0.034 - ETA: 2:10 - loss: 4.4741 - acc: 0.034 - ETA: 2:07 - loss: 4.4758 - acc: 0.034 - ETA: 2:04 - loss: 4.4775 - acc: 0.033 - ETA: 2:01 - loss: 4.4798 - acc: 0.033 - ETA: 1:59 - loss: 4.4835 - acc: 0.032 - ETA: 1:56 - loss: 4.4827 - acc: 0.033 - ETA: 1:53 - loss: 4.4793 - acc: 0.034 - ETA: 1:50 - loss: 4.4811 - acc: 0.033 - ETA: 1:47 - loss: 4.4817 - acc: 0.033 - ETA: 1:44 - loss: 4.4847 - acc: 0.032 - ETA: 1:42 - loss: 4.4823 - acc: 0.032 - ETA: 1:39 - loss: 4.4825 - acc: 0.033 - ETA: 1:36 - loss: 4.4814 - acc: 0.032 - ETA: 1:33 - loss: 4.4797 - acc: 0.032 - ETA: 1:30 - loss: 4.4752 - acc: 0.032 - ETA: 1:27 - loss: 4.4719 - acc: 0.034 - ETA: 1:25 - loss: 4.4697 - acc: 0.033 - ETA: 1:22 - loss: 4.4657 - acc: 0.033 - ETA: 1:19 - loss: 4.4677 - acc: 0.034 - ETA: 1:16 - loss: 4.4715 - acc: 0.034 - ETA: 1:13 - loss: 4.4744 - acc: 0.034 - ETA: 1:11 - loss: 4.4702 - acc: 0.034 - ETA: 1:08 - loss: 4.4713 - acc: 0.033 - ETA: 1:05 - loss: 4.4686 - acc: 0.033 - ETA: 1:02 - loss: 4.4696 - acc: 0.033 - ETA: 59s - loss: 4.4692 - acc: 0.033 - ETA: 56s - loss: 4.4712 - acc: 0.03 - ETA: 54s - loss: 4.4676 - acc: 0.03 - ETA: 51s - loss: 4.4675 - acc: 0.03 - ETA: 48s - loss: 4.4638 - acc: 0.03 - ETA: 45s - loss: 4.4595 - acc: 0.03 - ETA: 42s - loss: 4.4588 - acc: 0.03 - ETA: 39s - loss: 4.4590 - acc: 0.03 - ETA: 37s - loss: 4.4585 - acc: 0.03 - ETA: 34s - loss: 4.4594 - acc: 0.03 - ETA: 31s - loss: 4.4587 - acc: 0.03 - ETA: 28s - loss: 4.4544 - acc: 0.03 - ETA: 25s - loss: 4.4541 - acc: 0.03 - ETA: 22s - loss: 4.4518 - acc: 0.03 - ETA: 19s - loss: 4.4537 - acc: 0.03 - ETA: 17s - loss: 4.4510 - acc: 0.03 - ETA: 14s - loss: 4.4484 - acc: 0.03 - ETA: 11s - loss: 4.4447 - acc: 0.03 - ETA: 8s - loss: 4.4445 - acc: 0.0376 - ETA: 5s - loss: 4.4439 - acc: 0.038 - ETA: 2s - loss: 4.4450 - acc: 0.037 - 322s 3s/step - loss: 4.4440 - acc: 0.0380 - val_loss: 4.3816 - val_acc: 0.0413\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.51640 to 4.38158, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - ETA: 5:17 - loss: 4.3088 - acc: 0.0000e+0 - ETA: 5:03 - loss: 4.4148 - acc: 0.0250    - ETA: 4:55 - loss: 4.4060 - acc: 0.033 - ETA: 4:49 - loss: 4.5293 - acc: 0.037 - ETA: 4:47 - loss: 4.5332 - acc: 0.030 - ETA: 4:45 - loss: 4.4850 - acc: 0.041 - ETA: 4:40 - loss: 4.4990 - acc: 0.042 - ETA: 4:36 - loss: 4.4541 - acc: 0.043 - ETA: 4:32 - loss: 4.4127 - acc: 0.044 - ETA: 4:28 - loss: 4.4003 - acc: 0.040 - ETA: 4:24 - loss: 4.3840 - acc: 0.040 - ETA: 4:20 - loss: 4.3881 - acc: 0.041 - ETA: 4:16 - loss: 4.3732 - acc: 0.042 - ETA: 4:13 - loss: 4.3726 - acc: 0.039 - ETA: 4:10 - loss: 4.3799 - acc: 0.040 - ETA: 4:08 - loss: 4.3867 - acc: 0.040 - ETA: 4:05 - loss: 4.3887 - acc: 0.041 - ETA: 4:02 - loss: 4.3930 - acc: 0.041 - ETA: 3:59 - loss: 4.3928 - acc: 0.039 - ETA: 3:56 - loss: 4.3977 - acc: 0.037 - ETA: 3:52 - loss: 4.3916 - acc: 0.035 - ETA: 3:49 - loss: 4.4032 - acc: 0.034 - ETA: 3:46 - loss: 4.3947 - acc: 0.034 - ETA: 3:43 - loss: 4.3876 - acc: 0.033 - ETA: 3:39 - loss: 4.3848 - acc: 0.032 - ETA: 3:37 - loss: 4.3901 - acc: 0.030 - ETA: 3:34 - loss: 4.3829 - acc: 0.029 - ETA: 3:30 - loss: 4.3802 - acc: 0.032 - ETA: 3:27 - loss: 4.3771 - acc: 0.034 - ETA: 3:24 - loss: 4.3818 - acc: 0.036 - ETA: 3:21 - loss: 4.3781 - acc: 0.038 - ETA: 3:18 - loss: 4.3725 - acc: 0.039 - ETA: 3:15 - loss: 4.3793 - acc: 0.039 - ETA: 3:12 - loss: 4.3750 - acc: 0.041 - ETA: 3:09 - loss: 4.3815 - acc: 0.041 - ETA: 3:06 - loss: 4.3803 - acc: 0.041 - ETA: 3:03 - loss: 4.3910 - acc: 0.041 - ETA: 3:00 - loss: 4.3907 - acc: 0.044 - ETA: 2:57 - loss: 4.3862 - acc: 0.044 - ETA: 2:55 - loss: 4.3944 - acc: 0.045 - ETA: 2:52 - loss: 4.3951 - acc: 0.045 - ETA: 2:49 - loss: 4.3906 - acc: 0.044 - ETA: 2:46 - loss: 4.3888 - acc: 0.043 - ETA: 2:43 - loss: 4.3829 - acc: 0.043 - ETA: 2:40 - loss: 4.3737 - acc: 0.043 - ETA: 2:37 - loss: 4.3689 - acc: 0.042 - ETA: 2:34 - loss: 4.3654 - acc: 0.043 - ETA: 2:31 - loss: 4.3656 - acc: 0.044 - ETA: 2:28 - loss: 4.3697 - acc: 0.043 - ETA: 2:25 - loss: 4.3733 - acc: 0.044 - ETA: 2:22 - loss: 4.3594 - acc: 0.043 - ETA: 2:19 - loss: 4.3594 - acc: 0.042 - ETA: 2:16 - loss: 4.3658 - acc: 0.041 - ETA: 2:13 - loss: 4.3659 - acc: 0.042 - ETA: 2:10 - loss: 4.3617 - acc: 0.044 - ETA: 2:07 - loss: 4.3607 - acc: 0.044 - ETA: 2:05 - loss: 4.3647 - acc: 0.045 - ETA: 2:02 - loss: 4.3630 - acc: 0.044 - ETA: 1:59 - loss: 4.3642 - acc: 0.044 - ETA: 1:56 - loss: 4.3645 - acc: 0.045 - ETA: 1:53 - loss: 4.3624 - acc: 0.046 - ETA: 1:50 - loss: 4.3588 - acc: 0.047 - ETA: 1:47 - loss: 4.3627 - acc: 0.047 - ETA: 1:44 - loss: 4.3646 - acc: 0.047 - ETA: 1:41 - loss: 4.3655 - acc: 0.048 - ETA: 1:39 - loss: 4.3678 - acc: 0.048 - ETA: 1:36 - loss: 4.3586 - acc: 0.050 - ETA: 1:33 - loss: 4.3604 - acc: 0.050 - ETA: 1:30 - loss: 4.3601 - acc: 0.049 - ETA: 1:27 - loss: 4.3578 - acc: 0.048 - ETA: 1:24 - loss: 4.3622 - acc: 0.047 - ETA: 1:21 - loss: 4.3618 - acc: 0.047 - ETA: 1:18 - loss: 4.3536 - acc: 0.048 - ETA: 1:15 - loss: 4.3541 - acc: 0.048 - ETA: 1:12 - loss: 4.3559 - acc: 0.048 - ETA: 1:09 - loss: 4.3532 - acc: 0.048 - ETA: 1:06 - loss: 4.3505 - acc: 0.049 - ETA: 1:03 - loss: 4.3544 - acc: 0.050 - ETA: 1:00 - loss: 4.3525 - acc: 0.050 - ETA: 58s - loss: 4.3505 - acc: 0.049 - ETA: 55s - loss: 4.3463 - acc: 0.05 - ETA: 52s - loss: 4.3457 - acc: 0.05 - ETA: 49s - loss: 4.3434 - acc: 0.04 - ETA: 46s - loss: 4.3403 - acc: 0.04 - ETA: 43s - loss: 4.3391 - acc: 0.04 - ETA: 40s - loss: 4.3334 - acc: 0.05 - ETA: 37s - loss: 4.3287 - acc: 0.05 - ETA: 34s - loss: 4.3313 - acc: 0.04 - ETA: 31s - loss: 4.3304 - acc: 0.05 - ETA: 28s - loss: 4.3280 - acc: 0.05 - ETA: 26s - loss: 4.3246 - acc: 0.05 - ETA: 23s - loss: 4.3203 - acc: 0.05 - ETA: 20s - loss: 4.3209 - acc: 0.05 - ETA: 17s - loss: 4.3171 - acc: 0.05 - ETA: 14s - loss: 4.3174 - acc: 0.05 - ETA: 11s - loss: 4.3141 - acc: 0.05 - ETA: 8s - loss: 4.3152 - acc: 0.0515 - ETA: 5s - loss: 4.3141 - acc: 0.052 - ETA: 2s - loss: 4.3100 - acc: 0.052 - 324s 3s/step - loss: 4.3138 - acc: 0.0520 - val_loss: 4.3235 - val_acc: 0.0471\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.38158 to 4.32351, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - ETA: 5:39 - loss: 4.3705 - acc: 0.050 - ETA: 5:09 - loss: 4.2925 - acc: 0.025 - ETA: 4:55 - loss: 4.2786 - acc: 0.050 - ETA: 4:47 - loss: 4.3048 - acc: 0.050 - ETA: 4:42 - loss: 4.2670 - acc: 0.050 - ETA: 4:37 - loss: 4.2270 - acc: 0.041 - ETA: 4:32 - loss: 4.1881 - acc: 0.057 - ETA: 4:33 - loss: 4.1740 - acc: 0.062 - ETA: 4:31 - loss: 4.1871 - acc: 0.066 - ETA: 4:27 - loss: 4.2107 - acc: 0.070 - ETA: 4:23 - loss: 4.2227 - acc: 0.063 - ETA: 4:19 - loss: 4.2407 - acc: 0.062 - ETA: 4:16 - loss: 4.2456 - acc: 0.061 - ETA: 4:12 - loss: 4.2514 - acc: 0.057 - ETA: 4:09 - loss: 4.2832 - acc: 0.053 - ETA: 4:06 - loss: 4.2766 - acc: 0.053 - ETA: 4:03 - loss: 4.2724 - acc: 0.055 - ETA: 3:57 - loss: 4.2605 - acc: 0.062 - ETA: 3:54 - loss: 4.2394 - acc: 0.064 - ETA: 3:51 - loss: 4.2262 - acc: 0.066 - ETA: 3:48 - loss: 4.2227 - acc: 0.067 - ETA: 3:45 - loss: 4.2298 - acc: 0.067 - ETA: 3:42 - loss: 4.2526 - acc: 0.066 - ETA: 3:39 - loss: 4.2603 - acc: 0.065 - ETA: 3:36 - loss: 4.2637 - acc: 0.069 - ETA: 3:33 - loss: 4.2588 - acc: 0.068 - ETA: 3:30 - loss: 4.2700 - acc: 0.065 - ETA: 3:27 - loss: 4.2662 - acc: 0.065 - ETA: 3:24 - loss: 4.2679 - acc: 0.063 - ETA: 3:21 - loss: 4.2704 - acc: 0.064 - ETA: 3:18 - loss: 4.2740 - acc: 0.065 - ETA: 3:15 - loss: 4.2816 - acc: 0.063 - ETA: 3:13 - loss: 4.2839 - acc: 0.062 - ETA: 3:10 - loss: 4.2796 - acc: 0.064 - ETA: 3:07 - loss: 4.2832 - acc: 0.062 - ETA: 3:04 - loss: 4.2822 - acc: 0.061 - ETA: 3:01 - loss: 4.2852 - acc: 0.065 - ETA: 2:58 - loss: 4.2864 - acc: 0.065 - ETA: 2:55 - loss: 4.2816 - acc: 0.067 - ETA: 2:52 - loss: 4.2847 - acc: 0.065 - ETA: 2:49 - loss: 4.2846 - acc: 0.065 - ETA: 2:46 - loss: 4.2835 - acc: 0.064 - ETA: 2:43 - loss: 4.2896 - acc: 0.064 - ETA: 2:40 - loss: 4.2851 - acc: 0.063 - ETA: 2:37 - loss: 4.2872 - acc: 0.063 - ETA: 2:35 - loss: 4.2838 - acc: 0.064 - ETA: 2:32 - loss: 4.2853 - acc: 0.065 - ETA: 2:29 - loss: 4.2882 - acc: 0.065 - ETA: 2:26 - loss: 4.2839 - acc: 0.065 - ETA: 2:23 - loss: 4.2840 - acc: 0.065 - ETA: 2:20 - loss: 4.2877 - acc: 0.064 - ETA: 2:17 - loss: 4.2914 - acc: 0.064 - ETA: 2:14 - loss: 4.2902 - acc: 0.064 - ETA: 2:12 - loss: 4.2860 - acc: 0.063 - ETA: 2:09 - loss: 4.2856 - acc: 0.063 - ETA: 2:06 - loss: 4.2895 - acc: 0.062 - ETA: 2:03 - loss: 4.2937 - acc: 0.061 - ETA: 2:00 - loss: 4.2928 - acc: 0.062 - ETA: 1:57 - loss: 4.2961 - acc: 0.062 - ETA: 1:54 - loss: 4.2974 - acc: 0.062 - ETA: 1:51 - loss: 4.2982 - acc: 0.061 - ETA: 1:48 - loss: 4.2933 - acc: 0.060 - ETA: 1:46 - loss: 4.2933 - acc: 0.060 - ETA: 1:43 - loss: 4.2941 - acc: 0.062 - ETA: 1:40 - loss: 4.2940 - acc: 0.061 - ETA: 1:37 - loss: 4.2953 - acc: 0.061 - ETA: 1:34 - loss: 4.2964 - acc: 0.061 - ETA: 1:31 - loss: 4.2882 - acc: 0.062 - ETA: 1:28 - loss: 4.2925 - acc: 0.062 - ETA: 1:26 - loss: 4.2940 - acc: 0.063 - ETA: 1:23 - loss: 4.2958 - acc: 0.063 - ETA: 1:20 - loss: 4.2982 - acc: 0.062 - ETA: 1:17 - loss: 4.2960 - acc: 0.064 - ETA: 1:14 - loss: 4.2938 - acc: 0.065 - ETA: 1:11 - loss: 4.2907 - acc: 0.064 - ETA: 1:08 - loss: 4.2887 - acc: 0.063 - ETA: 1:05 - loss: 4.2942 - acc: 0.063 - ETA: 1:03 - loss: 4.2947 - acc: 0.063 - ETA: 1:00 - loss: 4.2948 - acc: 0.062 - ETA: 57s - loss: 4.3001 - acc: 0.061 - ETA: 54s - loss: 4.2993 - acc: 0.06 - ETA: 51s - loss: 4.3013 - acc: 0.06 - ETA: 48s - loss: 4.2993 - acc: 0.06 - ETA: 45s - loss: 4.2993 - acc: 0.06 - ETA: 43s - loss: 4.2973 - acc: 0.06 - ETA: 40s - loss: 4.2979 - acc: 0.06 - ETA: 37s - loss: 4.2971 - acc: 0.06 - ETA: 34s - loss: 4.2945 - acc: 0.06 - ETA: 31s - loss: 4.2979 - acc: 0.06 - ETA: 28s - loss: 4.3009 - acc: 0.05 - ETA: 25s - loss: 4.3008 - acc: 0.05 - ETA: 22s - loss: 4.3019 - acc: 0.05 - ETA: 20s - loss: 4.3018 - acc: 0.05 - ETA: 17s - loss: 4.3018 - acc: 0.05 - ETA: 14s - loss: 4.3024 - acc: 0.05 - ETA: 11s - loss: 4.3018 - acc: 0.05 - ETA: 8s - loss: 4.2984 - acc: 0.0580 - ETA: 5s - loss: 4.2995 - acc: 0.057 - ETA: 2s - loss: 4.2953 - acc: 0.057 - 322s 3s/step - loss: 4.2922 - acc: 0.0578 - val_loss: 4.3887 - val_acc: 0.0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss did not improve from 4.32351\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - ETA: 6:00 - loss: 4.4570 - acc: 0.0000e+0 - ETA: 5:15 - loss: 4.2911 - acc: 0.0750    - ETA: 5:00 - loss: 4.2683 - acc: 0.066 - ETA: 4:52 - loss: 4.3261 - acc: 0.062 - ETA: 4:45 - loss: 4.2942 - acc: 0.050 - ETA: 4:39 - loss: 4.3029 - acc: 0.050 - ETA: 4:35 - loss: 4.3465 - acc: 0.042 - ETA: 4:32 - loss: 4.3219 - acc: 0.050 - ETA: 4:29 - loss: 4.3148 - acc: 0.050 - ETA: 4:28 - loss: 4.3077 - acc: 0.050 - ETA: 4:26 - loss: 4.3207 - acc: 0.045 - ETA: 4:22 - loss: 4.3261 - acc: 0.045 - ETA: 4:21 - loss: 4.2954 - acc: 0.046 - ETA: 4:19 - loss: 4.2884 - acc: 0.042 - ETA: 4:19 - loss: 4.2932 - acc: 0.043 - ETA: 4:16 - loss: 4.3005 - acc: 0.040 - ETA: 4:13 - loss: 4.3019 - acc: 0.038 - ETA: 4:09 - loss: 4.3169 - acc: 0.038 - ETA: 4:05 - loss: 4.3307 - acc: 0.036 - ETA: 4:01 - loss: 4.3533 - acc: 0.037 - ETA: 3:58 - loss: 4.3450 - acc: 0.040 - ETA: 3:55 - loss: 4.3411 - acc: 0.038 - ETA: 3:51 - loss: 4.3377 - acc: 0.037 - ETA: 3:48 - loss: 4.3418 - acc: 0.037 - ETA: 3:44 - loss: 4.3484 - acc: 0.036 - ETA: 3:41 - loss: 4.3390 - acc: 0.036 - ETA: 3:38 - loss: 4.3368 - acc: 0.037 - ETA: 3:34 - loss: 4.3360 - acc: 0.039 - ETA: 3:31 - loss: 4.3479 - acc: 0.039 - ETA: 3:27 - loss: 4.3441 - acc: 0.038 - ETA: 3:24 - loss: 4.3332 - acc: 0.041 - ETA: 3:21 - loss: 4.3262 - acc: 0.043 - ETA: 3:18 - loss: 4.3253 - acc: 0.042 - ETA: 3:14 - loss: 4.3354 - acc: 0.041 - ETA: 3:11 - loss: 4.3346 - acc: 0.040 - ETA: 3:08 - loss: 4.3335 - acc: 0.040 - ETA: 3:05 - loss: 4.3395 - acc: 0.040 - ETA: 3:02 - loss: 4.3403 - acc: 0.039 - ETA: 2:59 - loss: 4.3436 - acc: 0.039 - ETA: 2:56 - loss: 4.3432 - acc: 0.041 - ETA: 2:53 - loss: 4.3400 - acc: 0.041 - ETA: 2:50 - loss: 4.3351 - acc: 0.042 - ETA: 2:47 - loss: 4.3365 - acc: 0.043 - ETA: 2:44 - loss: 4.3387 - acc: 0.042 - ETA: 2:41 - loss: 4.3421 - acc: 0.042 - ETA: 2:38 - loss: 4.3354 - acc: 0.042 - ETA: 2:35 - loss: 4.3325 - acc: 0.042 - ETA: 2:32 - loss: 4.3304 - acc: 0.041 - ETA: 2:30 - loss: 4.3209 - acc: 0.041 - ETA: 2:27 - loss: 4.3179 - acc: 0.042 - ETA: 2:24 - loss: 4.3242 - acc: 0.041 - ETA: 2:21 - loss: 4.3173 - acc: 0.043 - ETA: 2:18 - loss: 4.3210 - acc: 0.042 - ETA: 2:15 - loss: 4.3194 - acc: 0.043 - ETA: 2:12 - loss: 4.3192 - acc: 0.043 - ETA: 2:09 - loss: 4.3230 - acc: 0.042 - ETA: 2:06 - loss: 4.3246 - acc: 0.043 - ETA: 2:04 - loss: 4.3230 - acc: 0.044 - ETA: 2:01 - loss: 4.3221 - acc: 0.044 - ETA: 1:58 - loss: 4.3163 - acc: 0.045 - ETA: 1:55 - loss: 4.3158 - acc: 0.045 - ETA: 1:52 - loss: 4.3168 - acc: 0.045 - ETA: 1:50 - loss: 4.3141 - acc: 0.045 - ETA: 1:47 - loss: 4.3195 - acc: 0.044 - ETA: 1:44 - loss: 4.3226 - acc: 0.044 - ETA: 1:40 - loss: 4.3181 - acc: 0.045 - ETA: 1:37 - loss: 4.3222 - acc: 0.044 - ETA: 1:34 - loss: 4.3188 - acc: 0.044 - ETA: 1:31 - loss: 4.3199 - acc: 0.043 - ETA: 1:28 - loss: 4.3140 - acc: 0.044 - ETA: 1:25 - loss: 4.3097 - acc: 0.045 - ETA: 1:22 - loss: 4.3078 - acc: 0.046 - ETA: 1:19 - loss: 4.3118 - acc: 0.046 - ETA: 1:16 - loss: 4.3122 - acc: 0.047 - ETA: 1:13 - loss: 4.3093 - acc: 0.047 - ETA: 1:10 - loss: 4.3077 - acc: 0.046 - ETA: 1:07 - loss: 4.3071 - acc: 0.046 - ETA: 1:04 - loss: 4.3098 - acc: 0.046 - ETA: 1:01 - loss: 4.3093 - acc: 0.046 - ETA: 58s - loss: 4.3049 - acc: 0.048 - ETA: 55s - loss: 4.3053 - acc: 0.04 - ETA: 52s - loss: 4.2997 - acc: 0.04 - ETA: 49s - loss: 4.2939 - acc: 0.05 - ETA: 47s - loss: 4.2917 - acc: 0.05 - ETA: 44s - loss: 4.2892 - acc: 0.05 - ETA: 41s - loss: 4.2868 - acc: 0.05 - ETA: 38s - loss: 4.2867 - acc: 0.05 - ETA: 35s - loss: 4.2889 - acc: 0.05 - ETA: 32s - loss: 4.2875 - acc: 0.05 - ETA: 29s - loss: 4.2900 - acc: 0.05 - ETA: 26s - loss: 4.2901 - acc: 0.05 - ETA: 23s - loss: 4.2918 - acc: 0.05 - ETA: 20s - loss: 4.2956 - acc: 0.05 - ETA: 17s - loss: 4.2934 - acc: 0.05 - ETA: 14s - loss: 4.2908 - acc: 0.05 - ETA: 11s - loss: 4.2948 - acc: 0.05 - ETA: 8s - loss: 4.2915 - acc: 0.0536 - ETA: 5s - loss: 4.2896 - acc: 0.053 - ETA: 2s - loss: 4.2834 - acc: 0.055 - 327s 3s/step - loss: 4.2841 - acc: 0.0550 - val_loss: 4.2680 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.32351 to 4.26800, saving model to out_trang/weights.newbestaugmented.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x80becb0ba8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN from scratch\n",
    "model = create_cnn_model_trang()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='out_trang/weights.newbestaugmented.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "batch_size = 20\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000// batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=800 // batch_size,\n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "# Get the InceptionV3 model as the base model for training\n",
    "base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(224, 224, 3))\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# Add a fully-connected layer and a logistic layer \n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(120, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "# first: train only the top layers i.e. freeze all convolutional InceptionV3 layers\n",
    "# for layer in base_model.layers:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='out_trang/weights.pretrained.hdf5', \n",
    "                               save_best_only=True)\n",
    "# Compile with Adam\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit_generator(train_generator,\n",
    "                      steps_per_epoch = 818,\n",
    "                      validation_data = val_generator,\n",
    "                      validation_steps = 205,\n",
    "                      epochs = 20,\n",
    "                      verbose = 2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [02:04<00:00, 80.91it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-08866a9baa7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m299\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m299\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Make it an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the sample submission file to set up the test data - x_test\n",
    "test_data = pd.read_csv('../sample_submission.csv')\n",
    "# Create the x_test\n",
    "x_test = []\n",
    "for i in tqdm(test_data['id'].values):\n",
    "    img = cv2.imread('../test/{}.jpg'.format(i))\n",
    "    x_test.append(cv2.resize(img, (224, 224)))\n",
    "# Turn into an array\n",
    "x_test = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict x_test\n",
    "predictions = model.predict(x_test, verbose=2)\n",
    "# Set column names to those generated by the one-hot encoding \n",
    "one_hot_ys = pd.get_dummies(training_data[\"breed\"], sparse = True)\n",
    "col_names = one_hot_ys.columns.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
